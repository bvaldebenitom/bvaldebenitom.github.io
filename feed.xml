<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://bvaldebenitom.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://bvaldebenitom.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-17T23:10:35+00:00</updated><id>https://bvaldebenitom.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">On the importance of assessing the results of computational tools with ground truth data</title><link href="https://bvaldebenitom.github.io/blog/2025/groundtruth/" rel="alternate" type="text/html" title="On the importance of assessing the results of computational tools with ground truth data"/><published>2025-10-17T05:55:00+00:00</published><updated>2025-10-17T05:55:00+00:00</updated><id>https://bvaldebenitom.github.io/blog/2025/groundtruth</id><content type="html" xml:base="https://bvaldebenitom.github.io/blog/2025/groundtruth/"><![CDATA[<p><strong>This is a supplementary material for my upcoming book chapter “Transposable Element analysis in OMICS data”</strong></p> <h1 id="preliminaries">PRELIMINARIES</h1> <p><a href="https://bvaldebenitom.github.io/blog/2024/igv_coverage/">In a previous post</a>, I briefly mentioned how the tool SQuIRE, which can be used for locus-specific measurements of Transposable Element (TE) expression from RNA-Seq data, can results in false positives. Here, I will expand on that, by showing a fully reproducible example of this. This post is not intended to discredit SQuIRE, but rather to show that for any computational tool we always need to think on ways to test their results with ground truth data, that is, with inputs that we know for a fact that should result in a specific output. In this line, here I designed a simple experiment to show how this can be achieved.</p> <h1 id="what-you-will-need">WHAT YOU WILL NEED</h1> <ul> <li>A STAR-indexed genome</li> <li>Experiment sequences</li> <li>ART read simulator</li> <li>SQuIRE</li> <li>IGV</li> </ul> <h1 id="step-1---genome-index">STEP 1 - GENOME INDEX</h1> <p>The first step is to get the genome FASTA file and GTF gene annotation file:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget http://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz
wget http://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/genes/hg38.ncbiRefSeq.gtf.gz

<span class="nb">gzip</span> <span class="nt">-d</span> hg38.fa.gz
<span class="nb">gzip</span> <span class="nt">-d</span> hg38.ncbiRefSeq.gtf.gz
</code></pre></div></div> <p>We can then build the index with STAR as follows:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>STAR   <span class="nt">--runMode</span> genomeGenerate      <span class="nt">--runThreadN</span> 21   <span class="nt">--genomeDir</span> hg38_STAR_INDEX  <span class="nt">--genomeFastaFiles</span> hg38.fa      <span class="nt">--sjdbGTFfile</span> hg38.ncbiRefSeq.gtf
</code></pre></div></div> <h1 id="step-2---experiment-sequences">STEP 2 - EXPERIMENT SEQUENCES</h1> <p>For this simple experiment, in the <code class="language-plaintext highlighter-rouge">experiment_sequences.fa</code> file, we only have two sequences:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">grep</span> <span class="s2">"^&gt;"</span> experiment_sequences.fa
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## &gt;chr4|4013499|4019703|L1PA4:L1:LINE|5.8|-::chr4:4013499-4019703(-)
## &gt;NM_020364.4 Homo sapiens deleted in azoospermia 3 (DAZ3), mRNA
</code></pre></div></div> <p>One is a sequence from a TE <strong>located in chromosome 4</strong>, while the other is from the protein-coding sequence of the DAZ3 gene <strong>located in chromosome Y</strong>.</p> <h1 id="step-3---read-simulation">STEP 3 - READ SIMULATION</h1> <p>We can get the ART read simulator from <a href="https://www.niehs.nih.gov/research/resources/software/biostatistics/art">from its official website</a>:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget https://www.niehs.nih.gov/sites/default/files/2024-02/artbinmountrainier2016.06.05linux64.tgz
<span class="nb">tar</span> <span class="nt">-xvf</span> artbinmountrainier2016.06.05linux64.tgz 
</code></pre></div></div> <p>To perform read simulation, we set then <code class="language-plaintext highlighter-rouge">-ss HS25</code> for single-end layout, <code class="language-plaintext highlighter-rouge">-l 150</code> for read length of 150 bp, <code class="language-plaintext highlighter-rouge">-f 30</code> to simulate reads representing 30X coverage of the sequences in <code class="language-plaintext highlighter-rouge">-i experiment_sequences.fa</code>, and <code class="language-plaintext highlighter-rouge">-o experiment_sequences</code> as output prefix.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>art_bin_MountRainier/art_illumina <span class="nt">-ss</span> HS25 <span class="nt">-i</span> experiment_sequences.fa <span class="nt">-l</span> 150 <span class="nt">-f</span> 30 <span class="nt">-o</span> experiment_sequences
</code></pre></div></div> <p>This will give us the <code class="language-plaintext highlighter-rouge">experiment_sequences.fq</code> file, which has the FASTQ simulated reads to map against the human genome.</p> <h1 id="step-4---setting-up-squire">STEP 4 - SETTING UP SQUIRE</h1> <p>The installation of SQuIRE is a relatively <em>seamless</em> process if you already have Conda. Their repository is well documented and provides instruction to install the tool from scratch, including setting up Conda. You can check their instructions <a href="https://github.com/wyang17/SQuIRE/">here</a></p> <p>Once installed, we need to check that the <code class="language-plaintext highlighter-rouge">Count</code> tool is available:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>squire Count <span class="nt">-h</span>

usage: squire Count <span class="o">[</span><span class="nt">-h</span><span class="o">]</span> <span class="o">[</span><span class="nt">-m</span> &lt;folder&gt;] <span class="o">[</span><span class="nt">-c</span> &lt;folder&gt;] <span class="o">[</span><span class="nt">-o</span> &lt;folder&gt;]
                    <span class="o">[</span><span class="nt">-t</span> &lt;folder&gt;] <span class="o">[</span><span class="nt">-f</span> &lt;folder&gt;] <span class="nt">-r</span> &lt;int&gt; <span class="o">[</span><span class="nt">-n</span> &lt;str&gt;]
                    <span class="o">[</span><span class="nt">-b</span> &lt;build&gt;] <span class="o">[</span><span class="nt">-p</span> &lt;int&gt;] <span class="o">[</span><span class="nt">-s</span> &lt;int&gt;] <span class="o">[</span><span class="nt">-e</span> EM] <span class="o">[</span><span class="nt">-v</span><span class="o">]</span>

Arguments:
  <span class="nt">-h</span>, <span class="nt">--help</span>            show this <span class="nb">help </span>message and <span class="nb">exit</span>
  <span class="nt">-m</span> &lt;folder&gt;, <span class="nt">--map_folder</span> &lt;folder&gt;
                        Folder location of outputs from SQuIRE Map <span class="o">(</span>optional,
                        default <span class="o">=</span> <span class="s1">'squire_map'</span><span class="o">)</span>
  <span class="nt">-c</span> &lt;folder&gt;, <span class="nt">--clean_folder</span> &lt;folder&gt;
                        Folder location of outputs from SQuIRE Clean
                        <span class="o">(</span>optional, default <span class="o">=</span> <span class="s1">'squire_clean'</span><span class="o">)</span>
  <span class="nt">-o</span> &lt;folder&gt;, <span class="nt">--count_folder</span> &lt;folder&gt;
                        Destination folder <span class="k">for </span>output files<span class="o">(</span>optional, default
                        <span class="o">=</span> <span class="s1">'squire_count'</span><span class="o">)</span>
  <span class="nt">-t</span> &lt;folder&gt;, <span class="nt">--tempfolder</span> &lt;folder&gt;
                        Folder <span class="k">for </span>tempfiles <span class="o">(</span>optional<span class="p">;</span> <span class="nv">default</span><span class="o">=</span><span class="s1">'count_folder'</span><span class="o">)</span>
  <span class="nt">-f</span> &lt;folder&gt;, <span class="nt">--fetch_folder</span> &lt;folder&gt;
                        Folder location of outputs from SQuIRE Fetch
                        <span class="o">(</span>optional, default <span class="o">=</span> <span class="s1">'squire_fetch)'</span>
  <span class="nt">-r</span> &lt;int&gt;, <span class="nt">--read_length</span> &lt;int&gt;
                        Read length <span class="o">(</span><span class="k">if </span>trim3 selected, after trimming<span class="p">;</span>
                        required<span class="o">)</span><span class="nb">.</span>
  <span class="nt">-n</span> &lt;str&gt;, <span class="nt">--name</span> &lt;str&gt;
                        Common <span class="nb">basename </span><span class="k">for </span>input files <span class="o">(</span>required <span class="k">if </span>more than
                        one bam file <span class="k">in </span>map_folder<span class="o">)</span>
  <span class="nt">-b</span> &lt;build&gt;, <span class="nt">--build</span> &lt;build&gt;
                        UCSC designation <span class="k">for </span>genome build, eg. <span class="s1">'hg38'</span>
                        <span class="o">(</span>required <span class="k">if </span>more than 1 build <span class="k">in </span>clean_folder<span class="o">)</span>
  <span class="nt">-p</span> &lt;int&gt;, <span class="nt">--pthreads</span> &lt;int&gt;
                        Launch &lt;int&gt; parallel threads<span class="o">(</span>optional<span class="p">;</span> <span class="nv">default</span><span class="o">=</span><span class="s1">'1'</span><span class="o">)</span>
  <span class="nt">-s</span> &lt;int&gt;, <span class="nt">--strandedness</span> &lt;int&gt;
                        <span class="s1">'0'</span> <span class="k">if </span>unstranded eg Standard Illumina, 1 <span class="k">if </span>first-
                        strand eg Illumina Truseq, dUTP, NSR, NNSR, 2 <span class="k">if
                        </span>second-strand, eg Ligation, Standard SOLiD
                        <span class="o">(</span>optional,default<span class="o">=</span>0<span class="o">)</span>
  <span class="nt">-e</span> EM, <span class="nt">--EM</span> EM        Run estimation-maximization on TE counts given number
                        of <span class="nb">times</span> <span class="o">(</span>optional, specify 0 <span class="k">if </span>no EM desired<span class="p">;</span>
                        <span class="nv">default</span><span class="o">=</span>auto<span class="o">)</span>
  <span class="nt">-v</span>, <span class="nt">--verbosity</span>       Want messages and runtime printed to stderr <span class="o">(</span>optional<span class="p">;</span>
                        <span class="nv">default</span><span class="o">=</span>False<span class="o">)</span>

</code></pre></div></div> <p>We now need to run these commands to prepare some additional files and directories required for the quantification of TE expression:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>squire Fetch <span class="nt">-b</span> hg38 <span class="nt">-c</span> <span class="nt">-g</span> <span class="nt">-r</span> <span class="nt">-v</span>
squire Clean <span class="nt">-r</span> squire_fetch/hg38_rmsk.txt <span class="nt">-v</span>
</code></pre></div></div> <p>Now, we should have the directories <code class="language-plaintext highlighter-rouge">squire_fetch</code> and <code class="language-plaintext highlighter-rouge">squire_clean</code>:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tree squire_fetch
tree squire_clean
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## squire_fetch
## ├── hg38chromFa.tar.gz
## ├── hg38_chromInfo.txt
## ├── hg38_refGene.bed
## ├── hg38_refGene.genepred
## ├── hg38_refGene.gtf
## └── hg38_rmsk.txt
## 
## 0 directories, 6 files
## squire_clean
## ├── hg38_all.bed
## └── hg38_all_copies.txt
## 
## 0 directories, 2 files
</code></pre></div></div> <h1 id="step-5---mapping">STEP 5 - MAPPING</h1> <p>Now we are all set!</p> <p>We can map our simulated sequences <code class="language-plaintext highlighter-rouge">experiment_sequences.fq</code> against the genome index, using the same options that <code class="language-plaintext highlighter-rouge">squire Map</code> <a href="https://github.com/wyang17/SQuIRE/blob/master/squire/Map.py#L173">uses</a></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>STAR <span class="nt">--runThreadN</span> 1 <span class="nt">--clip3pNbases</span> 0 <span class="nt">--outFilterMultimapNmax</span> 100 <span class="nt">--winAnchorMultimapNmax</span> 100 <span class="nt">--genomeDir</span> hg38_STAR_INDEX <span class="nt">--readFilesIn</span> experiment_sequences.fq <span class="nt">--outFileNamePrefix</span> squire_map/experiment_alignment_ <span class="nt">--outSAMtype</span> BAM SortedByCoordinate <span class="nt">--outSAMattributes</span> All <span class="nt">--outSAMstrandField</span> intronMotif <span class="nt">--outSAMattrIHstart</span> 0 <span class="nt">--sjdbGTFfile</span> hg38.ncbiRefSeq.gtf <span class="nt">--twopassMode</span> Basic
</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tree squire_map
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## squire_map
## ├── experiment_alignment_Aligned.sortedByCoord.out.bam
## ├── experiment_alignment_Aligned.sortedByCoord.out.bam.bai
## ├── experiment_alignment_Log.final.out
## ├── experiment_alignment_Log.out
## ├── experiment_alignment_Log.progress.out
## ├── experiment_alignment_SJ.out.tab
## ├── experiment_alignment__STARgenome
## │   ├── exonGeTrInfo.tab
## │   ├── exonInfo.tab
## │   ├── geneInfo.tab
## │   ├── sjdbInfo.txt
## │   ├── sjdbList.fromGTF.out.tab
## │   ├── sjdbList.out.tab
## │   └── transcriptInfo.tab
## └── experiment_alignment__STARpass1
##     ├── Log.final.out
##     └── SJ.out.tab
## 
## 2 directories, 15 files
</code></pre></div></div> <h1 id="step-6---te-quantification-and-validation">STEP 6 - TE QUANTIFICATION AND VALIDATION</h1> <p>With the mapping result, we can run the <code class="language-plaintext highlighter-rouge">Count</code> quantification tool:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>squire Count <span class="nt">-m</span> squire_map/ <span class="nt">-r</span> 150 <span class="nt">-c</span> squire_clean/ <span class="nt">-v</span>
</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tree squire_count
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## squire_count
## ├── experiment_alignment_Aligned.sortedByCoord.out_abund.txt
## ├── experiment_alignment_Aligned.sortedByCoord.out.gtf
## ├── experiment_alignment_Aligned.sortedByCoord.out_refGenecounts.txt
## ├── experiment_alignment_Aligned.sortedByCoord.out_subFcounts.txt
## └── experiment_alignment_Aligned.sortedByCoord.out_TEcounts.txt
## 
## 0 directories, 5 files
</code></pre></div></div> <p>Of these files, we are interested in <code class="language-plaintext highlighter-rouge">experiment_alignment_Aligned.sortedByCoord.out_TEcounts.txt</code>, which provides the quantification of TEs. We can check the most important columns:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">awk</span> <span class="s1">'BEGIN{FS=OFS="\t"}{print $1,$2,$3,$4,$15,$16,$17}'</span> squire_count/experiment_alignment_Aligned.sortedByCoord.out_TEcounts.txt
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## tx_chr	tx_start	tx_stop	TE_ID	uniq_counts	tot_counts	tot_reads
## chr4	4013502	4019680	chr4|4013498|4019703|L1PA4:L1:LINE|58|-	1118	1183.90	1216
## chr8	96955960	96956120	chr8|96955434|96956163|L1PA4:L1:LINE|47|+	0	4.31	6
## chr10	22061677	22061836	chr10|22060724|22066859|L1PA5:L1:LINE|53|+	0	4.20	6
## chr4	9617631	9617845	chr4|9617608|9619322|L1PA4:L1:LINE|86|+	0	5.29	15
## chr10	82344563	82344736	chr10|82344062|82350086|L1PA2:L1:LINE|25|-	0	3.79	12
## chr3	112659647	112659820	chr3|112659127|112665164|L1PA3:L1:LINE|29|-	0	3.79	12
## chr5	62656507	62656735	chr5|62655618|62660602|L1P1:L1:LINE|57|+	0	3.34	14
## chr5	93766091	93766242	chr5|93761070|93767199|L1PA4:L1:LINE|55|+	0	1.73	2
## chr2	133466941	133467107	chr2|133465007|133471038|L1PA2:L1:LINE|29|-	0	1.53	5
## chr20	29366629	29366779	chr20|29366059|29372220|L1PA4:L1:LINE|54|-	1	1.00	1
## chr5	114891825	114891975	chr5|114891256|114895798|L1PA4:L1:LINE|49|-	1	1.00	1
## chrX	97578708	97578858	chrX|97578170|97584188|L1PA3:L1:LINE|25|-	1	1.00	1
## chrX	75693308	75693519	chrX|75690170|75694093|L1PA4:L1:LINE|58|-	0	1.24	15
## chr3	115315428	115315678	chr3|115312305|115318470|L1PA3:L1:LINE|34|-	0	1.45	22
## chr3	121455034	121455284	chr3|121452690|121458395|L1PA3:L1:LINE|39|+	0	1.45	22
## chr12	29982525	29982675	chr12|29981945|29982867|L1PA4:L1:LINE|29|-	0	0.50	1
## chr1	48118716	48118866	chr1|48118135|48120412|L1PA4:L1:LINE|31|-	0	0.50	1
## chrY	24773569	24784208	chrY|24776006|24782046|L1PA2:L1:LINE|31|+	15	15.00	15
</code></pre></div></div> <p><strong>Wait a minute!!</strong>. So, from our experiment design, we should only expect TE expression at chromosome 4, specifically at <code class="language-plaintext highlighter-rouge">chr4:4013499-4019703</code>. We indeed see this on the first row, but there is expression detected at other chromosomes:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">awk</span> <span class="s1">'BEGIN{FS=OFS="\t"}{print $1}'</span> squire_count/experiment_alignment_Aligned.sortedByCoord.out_TEcounts.txt|sort|uniq <span class="nt">-c</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>##       1 chr1
##       2 chr10
##       1 chr12
##       1 chr2
##       1 chr20
##       3 chr3
##       2 chr4
##       3 chr5
##       1 chr8
##       2 chrX
##       1 chrY
##       1 tx_chr
</code></pre></div></div> <p>We see a result in chromosome Y, at locations 24773569 to 24784208:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">awk</span> <span class="s1">'BEGIN{FS=OFS="\t"}{print $1,$2,$3,$4}'</span> squire_count/experiment_alignment_Aligned.sortedByCoord.out_TEcounts.txt|grep <span class="s2">"chrY"</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## chrY	24773569	24784208	chrY|24776006|24782046|L1PA2:L1:LINE|31|+
</code></pre></div></div> <p>This regions overlap with the location of DAZ3, approximately at <code class="language-plaintext highlighter-rouge">chrY:24761069-24815492</code>, which <strong>is the gene from which we also simulated sequences</strong>.</p> <p>We can now inspect these results in IGV. First, we create a <em>bedgraph</em> representation of our BAM file using the <code class="language-plaintext highlighter-rouge">bamCoverage</code> tool from <a href="https://deeptools.readthedocs.io/en/latest/">deepTools</a>:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bamCoverage <span class="nt">--bam</span> squire_map/experiment_alignment_Aligned.sortedByCoord.out.bam <span class="nt">--outFileName</span> experiment.bedgraph <span class="nt">--outFileFormat</span> bedgraph <span class="nt">--binSize</span> 5 <span class="nt">--region</span> chrY:24761069:24815492
</code></pre></div></div> <p>A dummy representation of SQuIRE’s results as <em>bedgraph</em> can be created as follows:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"chrY</span><span class="se">\t</span><span class="s2">24773569</span><span class="se">\t</span><span class="s2">24784208</span><span class="se">\t</span><span class="s2">15"</span> <span class="o">&gt;</span> squire.bedgraph
</code></pre></div></div> <p>Those 2 files can be <em>seamlessly</em> loaded in IGV, and we will be able to see this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/03_squire/igv_snapshot_squire-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/03_squire/igv_snapshot_squire-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/03_squire/igv_snapshot_squire-1400.webp"/> <img src="/assets/img/03_squire/igv_snapshot_squire.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Genomic snapshot of the region chrY:24761069-24815492 </div> <p>If proficient with <a href="https://github.com/showteeth/ggcoverage/">ggCoverage</a>, we can create a similar representation of the IGV result, as a fully <em>vectorial</em> image:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/03_squire/ggcoverage_squire-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/03_squire/ggcoverage_squire-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/03_squire/ggcoverage_squire-1400.webp"/> <img src="/assets/img/03_squire/ggcoverage_squire.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Genomic snapshot of the region chrY:24761069-24815492 with ggCoverage. </div> <p>I found that SQuIRE over-estimates intronic TE expression due to <a href="https://github.com/wyang17/SQuIRE/blob/master/squire/Count.py#L269">indiscriminate use of BEDtools</a>. When there are <em>spliced</em> alignments, BEDtools by default will convert the interval in the SAM/BAM file to start position plus length of CIGAR string. As an example, for a CIGAR string like 20M3000N80N, and alignment position at 1000000, it will create an interval of length 20+3000+80 = 3100, and end position at 1003100. We need to either use <code class="language-plaintext highlighter-rouge">bedtools bamtobed</code> with the <code class="language-plaintext highlighter-rouge">-split</code> option to create an intermediate file (which we can further check that correctly has split intervals), or use the <code class="language-plaintext highlighter-rouge">-split</code> option in <code class="language-plaintext highlighter-rouge">bedtools intersect</code> so it does not artifically creates intervals longer than the sequencing reads.</p> <p>This is why whenever I use a tool, I design what I call “ground truth” experiments, in which with a simple test dataset, I can accurately tell if the tool does what it is supposed to do. Although it is very exciting that nowadays bioinformatics tool become more readily available, we should always exercise caution in their use, as this can result in arriving at biological conclusions that might not be entirely true. For example, using SQuIRE, a <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC9220773/#sec2-biology-11-00826">previous article</a> made a point of “intron retention” affecting TE quantification. From my experience, and the experiment shown here, I could argue that there migh t be over-estimation of intronic TE expression, but that does not correspond to actual “intron retention”.</p>]]></content><author><name></name></author><category term="reviews"/><category term="rnaseq"/><category term="transposableelements"/><summary type="html"><![CDATA[A fully provenanced example on validating the results of a computational tool]]></summary></entry><entry><title type="html">Nextflow vs Snakemake</title><link href="https://bvaldebenitom.github.io/blog/2024/nextflow_vs_snakemake/" rel="alternate" type="text/html" title="Nextflow vs Snakemake"/><published>2024-03-06T05:55:00+00:00</published><updated>2024-03-06T05:55:00+00:00</updated><id>https://bvaldebenitom.github.io/blog/2024/nextflow_vs_snakemake</id><content type="html" xml:base="https://bvaldebenitom.github.io/blog/2024/nextflow_vs_snakemake/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Throughout my career, I have always enjoyed the development of pipelines as a mean to efficiently process hundreds of files efficiently. I usually do this directly on Bash, as these pipelines connect the inputs and outputs of different software. I have been able to publish some of these tools in several journals, and at this point I’m always wondering whether they will work in all environments for all users. Although I have made a conscious effort to make them reproducible, they still fail at some point. Another issue is with Bash pipelines, is that I always have to make blocks of “if-else” statements just to check whether the inputs and outputs were created, which is very cumbersome.</p> <p>During the last 2 years, I have come across Nextflow and Snakemake, which both seem to have the aim to provide an infrastructure for “reproducible and scalable” workflows. In early 2023 I started using Snakemake, and in practice it effectively seemed to provide a cleaner and more organized way to develop and run pipelines. In particular, if you need to run different tools that need their own Conda environment, with Snakemake you can seamlessly define such environments and it will automatically switch them accordingly at execution time. This, however, also seems to be one of its drawbacks as it needs Conda to do so, and this can take ages some times (the always there “Solving environment”.. Just type “conda so” in Google and you will see that the most searched phrase is related to this). Though there is a way to make it run with Mamba, which is faster, it can get confusing if you haven’t worked with Python and Conda environments before. On the other hand, Nextflow, as we will see later, seems simpler, and it doesn’t require jumping into the whole snakes universe of Conda-Mamba-Snakemake.</p> <p>To get more familiarized with both Nextflow and Snakemake, I adapted a simplified version of my RNA-Seq workflow from scratch to each of these frameworks. In this post I will be discussing some of my findings and what I would recommend for someone wanting to implement them.</p> <h2 id="the-workflow">The workflow</h2> <p>For this workflow, I simulated reads in FASTQ formats: 3 paired-end libraries and 1 single-end library. Then, these reads would need to be quality checked with FastQC, aligned against the human genome with STAR, and finally, processed with Telescope to get expression estimates for Transposable Elements:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/02_nextflow_vs_snakemake/figure01-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/02_nextflow_vs_snakemake/figure01-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/02_nextflow_vs_snakemake/figure01-1400.webp"/> <img src="/assets/img/02_nextflow_vs_snakemake/figure01.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>There are 2 considerations for the pipeline:</p> <ol> <li>It should automatically detect whether the libraries are single-end or paired-end.</li> <li>Telescope needs to be run on its own Conda environment (I have already built it), so it should be able to reuse the same existing environment.</li> </ol> <p>I will show the implementation first on Snakemake and latter on Nextflow.</p> <h3 id="snakemake-implementation">Snakemake implementation</h3> <h4 id="setup">Setup</h4> <p>The basic commands for <a href="https://snakemake.readthedocs.io/en/stable/getting_started/installation.html">installing Snakemake</a> are:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda install -n base -c conda-forge mamba
mamba create -c conda-forge -c bioconda -n snakemake snakemake
mamba activate snakemake
snakemake --help
</code></pre></div></div> <p>So, right off the bat, we already need to have Conda installed, and before installing Snakemake, they recommend to install Mamba <strong>using Conda</strong>. If you do this, you might lose a good amount of time. I recommend installing Mamba or Micromamba following their instructions. In my case, I chose to do this with Micromamba, which I started using thanks to a good friend. This is how the commands look:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"${SHELL}" &lt;(curl -L micro.mamba.pm/install.sh)
micromamba create -c conda-forge -c bioconda -n snakemake snakemake
micromamba activate snakemake
</code></pre></div></div> <h4 id="writing-the-workflow">Writing the workflow</h4> <p>First, we will define paths to the FastQC and STAR binaries and other parameters:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>params_fastqc = "Snakemake_Nextflow/FastQC/fastqc"
params_fastqc_memory = "10000"
params_fastqc_threads = 1

params_star_index = "STAR_GenomeIndex/hg38_STAR_2.7.11b"
params_star = "STAR_2.7.11b/Linux_x86_64/STAR"
params_star_threads = 21

params_telescope_gtf = "hg38_all.gtf"
</code></pre></div></div> <p>Next, we will set some variables to guide the different steps of the pipeline. Snakemake allows the use of Python commands, so I’m taking advantage of this. First, the “glob” library is imported, and the “samples” variable is created by listing all files with extension “fq”. Concurrently, through the use of a regular expression, we substitute the trailing _1 or _2 of the paired-end files. In the regular expression we add the quantifier “{0,1}” which makes it also match single-end files that just end on “.fq”, without a trailing _1 or _2.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import glob

samples = list(set([re.sub("(_[12]){0,1}.fq","",x) for x in glob.glob("<span class="err">*</span>.fq")]))
<span class="gh"># ['hg38_tes_random_n1000_simulated_pe_reads_l150_f15_SE', 'hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m1000_s10', 'hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m1100_s10', 'hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m900_s10']</span>
</code></pre></div></div> <p>Similarly, the “fastq_basenames” variable is created:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fastq_basenames = [re.sub(".fq","",x) for x in glob.glob("<span class="err">*</span>.fq")]
<span class="gh"># ['hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m1100_s10_1', 'hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m1000_s10_2', 'hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m1100_s10_2', 'hg38_tes_random_n1000_simulated_pe_reads_l150_f15_SE', 'hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m900_s10_2', 'hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m1000_s10_1', 'hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m900_s10_1']</span>
</code></pre></div></div> <p>The variables “pe_samples” and “se_samples” follow a similar logic, with the exception that the first one will only contain the samples that are paired-end and the second one the ones that are single-end. These variables are used in the function “input_fastqs” which will process a Snakemake input and identify which type is the sample.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pe_samples = list(set([re.sub("(_[12]){0,1}.fq","",x) for x in glob.glob("<span class="err">*</span>_[12].fq")]))
<span class="gh"># ['hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m1000_s10', 'hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m1100_s10', 'hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m900_s10']</span>

se_samples = [re.sub(".fq","",x) for x in glob.glob("<span class="err">*</span>.fq") if re.search("[^_12].fq",x)]
<span class="gh"># ['hg38_tes_random_n1000_simulated_pe_reads_l150_f15_SE']</span>

def input_fastqs(wildcards):
        print(wildcards.sample)
        if wildcards.sample in se_samples:
                return(f'{wildcards.sample}.fq')
        if wildcards.sample in pe_samples:
                return([f'{wildcards.sample}_1.fq',f'{wildcards.sample}_2.fq'])
</code></pre></div></div> <p>Now we can get to the nitty-gritty. The Snakemake logic is that we define rules with inputs and outputs, and it automatically identifies which rule generates the required input of another rule. For example, it is common to create the rule “all” specifying the final outputs of a pipeline. In our case, it looks like this:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rule all:
        input: expand("snakemake_telescope/{sample}_telescope-telescope_report.tsv",sample=samples)
</code></pre></div></div> <p>“samples” is our variable defined above. With “expand” we are telling Snakemake to create a list containing filenames of the form “snakemake_telescope/<strong>{sample}</strong>_telescope-telescope_report.tsv” where sample can be “hg38_tes_random_n1000_simulated_pe_reads_l150_f15_SE” or “hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m1000_s10”. Since those do not exist at the beginning of the pipeline, it will identify the rule that creates those files as outputs. In our case, this is the rule “telescope”:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rule telescope:
        input: "snakemake_star/{sample}Aligned.out.bam"
        output: "snakemake_telescope/{sample}_telescope-telescope_report.tsv"
        conda: "telescope_env"
        shell: "telescope assign --outdir snakemake_telescope --exp_tag {wildcards.sample}_telescope {input} {params_telescope_gtf}"
</code></pre></div></div> <p>As we see, its output are in similar definition to the outputs of our rule “all”. The complete workflow definition is:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rule all:
        input: expand("snakemake_telescope/{sample}_telescope-telescope_report.tsv",sample=samples)

rule readqc:
        input: expand("{fastq_base}.fq",fastq_base=fastq_basenames)
        output: expand("snakemake_fastqc/{fastq_base}_fastqc.html",fastq_base=fastq_basenames)
        shell: "{params_fastqc} --memory {params_fastqc_memory} --threads {params_fastqc_threads} --outdir snakemake_fastqc {input}"

rule map:
        input: input_fastqs
        output: "snakemake_star/{sample}Aligned.out.bam"
        shell: "{params_star} --genomeDir {params_star_index} --runThreadN {params_star_threads} --readFilesIn {input} --outFileNamePrefix snakemake_star/{wildcards.sample} --outFilterMultimapNmax 100 --winAnchorMultimapNmax 100 --outSAMtype BAM Unsorted"

rule telescope:
        input: "snakemake_star/{sample}Aligned.out.bam"
        output: "snakemake_telescope/{sample}_telescope-telescope_report.tsv"
        conda: "telescope_env"
        shell: "telescope assign --outdir snakemake_telescope --exp_tag {wildcards.sample}_telescope {input} {params_telescope_gtf}"
</code></pre></div></div> <p>As discussed above, rule “all” is the main rule defining the final outputs, which are generated by “telescope”, which in turn requires BAM files generated by the “map” rule. Snakemake has the option “–dag”, which allows us to get a quick overview of the rule graph:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/02_nextflow_vs_snakemake/figure02_RNASeq_workflow_dag-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/02_nextflow_vs_snakemake/figure02_RNASeq_workflow_dag-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/02_nextflow_vs_snakemake/figure02_RNASeq_workflow_dag-1400.webp"/> <img src="/assets/img/02_nextflow_vs_snakemake/figure02_RNASeq_workflow_dag.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>It shows us that we have 4 samples processed by the “map” rule, and this goes to “telescope” and finally to “all”. Having this dependency between rules is what makes Snakemake a good option for developing pipelines: if the files are not generated, it will report an error, which we can debug. In contrast, doing this via Bash will entail several conditionals to ensure we are not overwriting successful runs and that we are properly handling interprocess dependencies. When I used Bash, I would end up creating random empty files because one process wouldn’t finish properly, and then the next one would start anyway.</p> <p>We can run the entire workflow like this:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>snakemake --snakefile RNASeq_workflow.smk --cores 1 all readqc --use-conda
</code></pre></div></div> <p>Both the “all” and the “readqc” rule are defined as targets, because we are not actually connecting anything with the FastQC outputs. This is why it appears without connections in the graph. Although we can make it connected to the mapping process, there is not an actual dependency in terms of inputs and outputs.</p> <h3 id="nextflow-implementation">Nextflow implementation</h3> <h4 id="setup-1">Setup</h4> <p>In the <a href="https://www.nextflow.io/">Nextflow homepage</a> it says that it requires “Zero config” and “Just download and play with it. No installation is required”. It requires Java, but other than that it can be set up with:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl -s https://get.nextflow.io | bash
</code></pre></div></div> <p>This doesn’t take more than 2 minutes, and indeed it requires zero installation. It creates a “nextflow” binary in the same directory where the command was run.</p> <h4 id="writing-the-workflow-1">Writing the workflow</h4> <p>Similar to the Snakemake example, I first set up some variables:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>params.fastqc = "Snakemake_Nextflow/FastQC/fastqc"
params.fastqc_memory = "10000"
params.fastqc_threads = 1

params.star_index = "STAR_GenomeIndex/hg38_STAR_2.7.11b"
params.star = "STAR_2.7.11b/Linux_x86_64/STAR"
params.star_threads = 21

params.telescope_gtf = "hg38_all.gtf"
</code></pre></div></div> <p>Instead of rules, we have “processes”. The logic in Nextflow is that a process can take an input and generate an output. The inputs and outputs are called “Channels”, and they can be of any type. For example, the “read_qc” process looks like this:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>process readqc {
        publishDir "nextflow_fastqc"<span class="sb">

        input: path fastq1_files
        output: path "${fastq1_files.baseName}_fastqc.html"

        "$params.fastqc --memory $params.fastqc_memory --threads $params.fastqc_threads $fastq1_files"
</span>}
</code></pre></div></div> <p>whereas the “align” process is:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>process align {
        publishDir "nextflow_star"<span class="sb">

        input: tuple val(basename),file(fastqfiles)
        output: path "${basename}Aligned.out.bam"

        "$params.star --genomeDir $params.star_index --runThreadN $params.star_threads --readFilesIn $fastqfiles --outFileNamePrefix ${basename} --outFilterMultimapNmax 100 --winAnchorMultimapNmax 100 --outSAMtype BAM Unsorted"
</span>}
</code></pre></div></div> <p>and the “telescope” process is:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>process telescope {
        conda '/home/ws2021/miniconda3/envs/telescope_env'
        publishDir "nextflow_telescope"<span class="sb">

        input: path bam_files
        output: val "${bam_files.baseName}_telescope"

        "telescope assign --exp_tag ${bam_files.baseName}_telescope $bam_files $params.telescope_gtf"
</span>}
</code></pre></div></div> <p>So, for “readqc” the inputs are FASTQ files and the outputs are the generated HTML files, which share the basename of the input FASTQ file. Then, in “align” we take as input a tuple, containing the sample identifier, and the FASTQ files (this way we can use the sample identifier for “outFileNamePrefix”), and the output is a BAM file. For “telescope” the input is the BAM file, but here I defined the output as a simple value.</p> <p>The workflow can be defined then as follows:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>workflow {
        PAIRS = Channel.fromFilePairs("<span class="err">*</span>_{1,2}.fq")
        SE = Channel.fromPath("<span class="err">*</span>.fq").filter { it.name =~ /[^12].fq/ }
        SE2 = SE.map { it -&gt; [it.simpleName,it]}<span class="sb">

        readqc(Channel.fromPath("*.fq"))|view
        align(PAIRS.concat(SE2))|telescope
</span>}
</code></pre></div></div> <p>Nextflow seems to have many ways to interact with sequencing data. For example, we can generate two file / path Channels in the first two lines. The first one uses “fromFilePairs” which is perfect for paired-end files. On the second, we use a “fromPath” to consider all files with “.fq” extension, and we filter it to remove those ending with “1” or “2”, effectively keeping single-end files. We process this variable with “map” to tranform it into a tuple:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>PAIRS = Channel.fromFilePairs("<span class="err">*</span>_{1,2}.fq")
<span class="gh">#[hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m1100_s10, [hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m1100_s10_1.fq, Snakemake_Nextflow/hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m1100_s10_2.fq]]</span>
<span class="gh">#[hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m1000_s10, [hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m1000_s10_1.fq, Snakemake_Nextflow/hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m1000_s10_2.fq]]</span>
<span class="gh">#[hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m900_s10, [hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m900_s10_1.fq, Snakemake_Nextflow/hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m900_s10_2.fq]]</span>

SE = Channel.fromPath("<span class="err">*</span>.fq").filter { it.name =~ /[^12].fq/ }
<span class="gh">#hg38_tes_random_n1000_simulated_pe_reads_l150_f15_SE.fq</span>

SE2 = SE.map { it -&gt; [it.simpleName,it]}
<span class="gh">#[hg38_tes_random_n1000_simulated_pe_reads_l150_f15_SE, hg38_tes_random_n1000_simulated_pe_reads_l150_f15_SE.fq]</span>
</code></pre></div></div> <p>This way, we can create a single variable that will allow for the proper processing of reads whether they are single- or paired-end. Altogether, the workflow definition is:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>workflow {
        PAIRS = Channel.fromFilePairs("<span class="err">*</span>_{1,2}.fq")
        SE = Channel.fromPath("<span class="err">*</span>.fq").filter { it.name =~ /[^12].fq/ }
        SE2 = SE.map { it -&gt; [it.simpleName,it]}<span class="sb">

        readqc(Channel.fromPath("*.fq"))|view
        align(PAIRS.concat(SE2))|telescope
</span>}
</code></pre></div></div> <p>Something that I really liked is the ability to pipe processes. Since “align” <em>emits</em> the BAM files required for “telescope”, we can just link them with the pipe.</p> <p>The Nextflow workflow can then be run with:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./nextflow RNASeq_workflow.nf -with-conda
</code></pre></div></div> <h2 id="comparison">Comparison</h2> <p>First things first, I have to say that I enjoyed using both Nextflow and Snakemake, and I wish I would’ve started using them sooner, as they are really convenient. Here, I adapted a really simplified version of what I usually do for RNA-Seq analysis so I could get an understanding on how each framework could be used and implemented. Also, it allowed me to make an unbiased and informed opinion on each one. For example, I didn’t have a very good opinion of Snakemake because according to my previous supervisor, it wasn’t able to use already existing Conda environments. Now I know better because I started to use it properly by carefully reading the documentation, the same way I do with any new thing that I start to learn.</p> <p>If you only plan to analyze sequencing data, I would probably recommend Nextflow because it has built-in features that enhance compatibility and handling this type of data. Setting Nextflow up has to be one of the fastest and hassle-free things that I have done. On the other hand, if you are not familiar with Java, you might face some issues in terms of syntax and how you can manipulate variables. I’m of the idea that once you know how to code in one language, switching to a new one can be fast. Still, I haven’t used Java in years, so I’m a bit rusty. Luckily, <a href="https://www.nextflow.io/docs/latest/script.html">they provide documentation to Groovy</a>, the specific name of the language, which helped me during this comparison. By contrast, in Snakemake you can use Python (which I learned over a year ago), and thus, for the functionalities that were missing I was able to quickly write some code to aid in the pipeline, as shown above.</p> <p>Another misconception that I had was the incompatibility of Nextflow with Conda environments. Nextflow allows for the seamless use and deploying of Conda environments, so if you are familiar with them, you are not really required to use Snakemake.</p> <p>Probably where Nextflow has the edge is in terms of scalability, outputs and reports. For scalability, its ease of use can be very convenient if you work in a cloud-computing environment or in a computer where you have limited capabilities to install software, such as Conda. I didn’t test it here, but I’m curious of their performance in AWS, for example. In terms of outputs, when you run the pipeline by default, Nextflow only shows the process and the completion percentage. On the other hand, Snakemake prints all the standard output of each rule, so you will see a similar output several times for each file processed in a rule, which will result in a messy terminal. Finally, Nextflow creates a beautiful and comprehensive report <strong>by just adding the -with-report OUTPUT_HTML</strong> option. You can see the report for this workflow <a href="https://bvaldebenitom.github.io/assets/html/rnaseq_report_nextflow.html">here</a>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/02_nextflow_vs_snakemake/figure03_nextflow_report-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/02_nextflow_vs_snakemake/figure03_nextflow_report-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/02_nextflow_vs_snakemake/figure03_nextflow_report-1400.webp"/> <img src="/assets/img/02_nextflow_vs_snakemake/figure03_nextflow_report.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>With this report you can quickly get an overview of several metrics of importance, such as CPU and memory usage per process and task, and as well as job duration. This can quickly help identify which processes are causing bottlenecks in the pipeline and if a process failed, whether it was caused by a mismatch in allocated memory and peak memory and so on. Overall, this report will go a long way in providing you with the detailed information that will help improve your pipelines.</p> <h2 id="summary">Summary</h2> <p>Here is a short summary of this comparison:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/02_nextflow_vs_snakemake/figure04_summary-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/02_nextflow_vs_snakemake/figure04_summary-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/02_nextflow_vs_snakemake/figure04_summary-1400.webp"/> <img src="/assets/img/02_nextflow_vs_snakemake/figure04_summary.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>I think that both frameworks are excellent and I recommend everyone to give them a try (or at least to one of them). However, given Nextflow’s ease of installation / minimum requirements, the ability to generate amazing reports with just a flag, and more compatibility straight out of the box with sequencing data, it will be my default recommendation in the meantime.</p>]]></content><author><name></name></author><category term="reviews"/><category term="workflows"/><category term="rnaseq"/><summary type="html"><![CDATA[A comparison of Nextflow and Snakemake in the context of RNA-Seq analysis]]></summary></entry><entry><title type="html">Automatization of IGV snapshots</title><link href="https://bvaldebenitom.github.io/blog/2024/igv_coverage/" rel="alternate" type="text/html" title="Automatization of IGV snapshots"/><published>2024-01-06T09:40:00+00:00</published><updated>2024-01-06T09:40:00+00:00</updated><id>https://bvaldebenitom.github.io/blog/2024/igv_coverage</id><content type="html" xml:base="https://bvaldebenitom.github.io/blog/2024/igv_coverage/"><![CDATA[<p>Ever since I started analyzing Transposable Elements (TEs) expression in RNA-Seq data, I have been curious to see the genomic coverage of specific loci. Moreover, when I found out that <a href="https://academic.oup.com/nar/article/47/5/e27/5280934">SQuIRE</a>, one of the best tools at the moment, could generate thousands of false positives, I was more aware of the need to manually verify the expression of TEs. Since there was a large amount of loci to check, I would always postpone this task, as I was never able to find an automated way of generating genomic coverage snapshots. Instead, after several statistical analyses, I would try to shorten the list of TEs to the smallest amount possible, so I could later check them one-by-one on the Integrative Genomics Viewer (IGV) application.</p> <p>It wasn’t until a few weeks ago that I decided to attempt this again. In turn, I was able to find this <a href="https://janbio.home.blog/2020/09/16/igv-batch-snapshot-from-command-line/">blog post</a> with the backbone of the solution. Since then, I modified it to suit my needs for RNA-Seq analyses, and decided to write this post as a more definite tutorial that goes from scratch to the final generation of these long-coveted automated IGV snapshots.</p> <h4 id="1-what-you-will-need">1. What you will need</h4> <ul> <li>Command line IGV 2.16.2 (https://igv.org/)</li> <li>Samtools (https://www.htslib.org/)</li> <li>A genome FASTA file (for this tutorial, I'm using the mm10 FASTA <a href="https://hgdownload.soe.ucsc.edu/goldenPath/mm10/bigZips/">from UCSC</a>)</li> <li>A GTF file (for this tutorial, I'm using the mm10 NCBI RefSeq <a href="https://hgdownload.soe.ucsc.edu/goldenPath/mm10/bigZips/genes/">from UCSC</a>)</li> <li>One or more BAM files (I'm using 2 BAM files here)</li> </ul> <h4 id="2-generating-the-snapshots">2. Generating the snapshots</h4> <p>There are two main steps to actually generating the IGV snapshots in an automated manner. First, we need to index the genome FASTA file, the genome annotation GTF file, and the BAM files.</p> <p>To make things simpler, I recommend creating an environment variable that contains the path to the IGV command line directory:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>export IGV_HOME=/path/IGV_2.16.2/
</code></pre></div></div> <p>Then, to index the genome FASTA file, we can do it like this:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$IGV_HOME/igvtools index mm10.fa
</code></pre></div></div> <p>For the GTF file, we need to sort it first, and then we can run the index command:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$IGV_HOME/igvtools sort mm10.ncbiRefSeq.gtf mm10.ncbiRefSeq_sorted.gtf
$IGV_HOME/igvtools index mm10.ncbiRefSeq_sorted.gtf
</code></pre></div></div> <p><strong>WARNING</strong>: Failure to index the GTF file, will result in long running times. Although the snapshot commands will work, when testing this tutorial on the unindexed file, it went for more than 26 minutes when loading it versus the ~1 minute it will take with the sorted and indexed version.</p> <p>Finally, to index the BAM files, we can use samtools:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>samtools index W8S1PA_Aligned.sortedByCoord.out.bam
samtools index W8S3SM_Aligned.sortedByCoord.out.bam
</code></pre></div></div> <p>The second step is creating the IGV script that will be used to actually generate the snapshots. Here is the template I use now:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gh">#load files</span>
genome mm10.fa 
load mm10.ncbiRefSeq_sorted.gtf 
 
preference SAM.SHOW_ALIGNMENT_TRACK false 

load W8S3SM_Aligned.sortedByCoord.out.bam 
load W8S1PA_Aligned.sortedByCoord.out.bam 
 
<span class="gh">#create snapshots </span>
snapshotDirectory . 

goto chr3:83,764,321-83,776,316 
snapshot igv_demo_sfrp2.png 
 
exit 
</code></pre></div></div> <p>We can save this into the file <code class="language-plaintext highlighter-rouge">igv_snapshot_demo.batch</code>. The script basically consists of loading the genome files (FASTA and sorted GTF), then setting <code class="language-plaintext highlighter-rouge">SAM.SHOW_ALIGNMENT_TRACK</code> to <code class="language-plaintext highlighter-rouge">false</code>, as I don’t usually need to check the reads track, and loading the BAM files. Later, we define the snapshot directory, and through the <code class="language-plaintext highlighter-rouge">goto</code> and <code class="language-plaintext highlighter-rouge">snapshot</code> commands we are actually creating them.</p> <p>Once all the above is done, we can run this final command to efficiently generate all the snapshots in one go:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>xvfb-run --auto-servernum --server-num=1 java -showversion --module-path="${IGV_HOME}/lib" -Xmx8g @"${IGV_HOME}/igv.args" --module=org.igv/org.broad.igv.ui.Main -b igv_snapshot_demo.batch
</code></pre></div></div> <p>In this example, the generated snapshot will be:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/igv_tutorial_sfrp2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/igv_tutorial_sfrp2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/igv_tutorial_sfrp2-1400.webp"/> <img src="/assets/img/igv_tutorial_sfrp2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Snapshot generated for the Sfrp2 gene using the commands in this tutorial. </div> <p>With this, we can now finally generate snapshots for a large number of genomic locations in an automated manner!</p>]]></content><author><name></name></author><category term="tutorials"/><category term="rnaseq"/><summary type="html"><![CDATA[A tutorial on best practices and how to do command line automatization of genomic coverage snapshots using the Integrative Genomics Viewer]]></summary></entry></feed>