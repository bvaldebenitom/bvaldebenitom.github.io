<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://bvaldebenitom.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://bvaldebenitom.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-03-17T13:53:59+00:00</updated><id>https://bvaldebenitom.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Nextflow vs Snakemake</title><link href="https://bvaldebenitom.github.io/blog/2024/nextflow_vs_snakemake/" rel="alternate" type="text/html" title="Nextflow vs Snakemake"/><published>2024-03-06T05:55:00+00:00</published><updated>2024-03-06T05:55:00+00:00</updated><id>https://bvaldebenitom.github.io/blog/2024/nextflow_vs_snakemake</id><content type="html" xml:base="https://bvaldebenitom.github.io/blog/2024/nextflow_vs_snakemake/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Throughout my career, I have always enjoyed the development of pipelines as a mean to efficiently process hundreds of files efficiently. I usually do this directly on Bash, as these pipelines connect the inputs and outputs of different software. I have been able to publish some of these tools in several journals, and at this point I’m always wondering whether they will work in all environments for all users. Although I have made a conscious effort to make them reproducible, they still fail at some point. Another issue is with Bash pipelines, is that I always have to make blocks of “if-else” statements just to check whether the inputs and outputs were created, which is very cumbersome.</p> <p>During the last 2 years, I have come across Nextflow and Snakemake, which both seem to have the aim to provide an infrastructure for “reproducible and scalable” workflows. In early 2023 I started using Snakemake, and in practice it effectively seemed to provide a cleaner and more organized way to develop and run pipelines. In particular, if you need to run different tools that need their own Conda environment, with Snakemake you can seamlessly define such environments and it will automatically switch them accordingly at execution time. This, however, also seems to be one of its drawbacks as it needs Conda to do so, and this can take ages some times (the always there “Solving environment”.. Just type “conda so” in Google and you will see that the most searched phrase is related to this). Though there is a way to make it run with Mamba, which is faster, it can get confusing if you haven’t worked with Python and Conda environments before. On the other hand, Nextflow, as we will see later, seems simpler, and it doesn’t require jumping into the whole snakes universe of Conda-Mamba-Snakemake.</p> <p>To get more familiarized with both Nextflow and Snakemake, I adapted a simplified version of my RNA-Seq workflow from scratch to each of these frameworks. In this post I will be discussing some of my findings and what I would recommend for someone wanting to implement them.</p> <h2 id="the-workflow">The workflow</h2> <p>For this workflow, I simulated reads in FASTQ formats: 3 paired-end libraries and 1 single-end library. Then, these reads would need to be quality checked with FastQC, aligned against the human genome with STAR, and finally, processed with Telescope to get expression estimates for Transposable Elements:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/02_nextflow_vs_snakemake/figure01-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/02_nextflow_vs_snakemake/figure01-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/02_nextflow_vs_snakemake/figure01-1400.webp"/> <img src="/assets/img/02_nextflow_vs_snakemake/figure01.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>There are 2 considerations for the pipeline:</p> <ol> <li>It should automatically detect whether the libraries are single-end or paired-end.</li> <li>Telescope needs to be run on its own Conda environment (I have already built it), so it should be able to reuse the same existing environment.</li> </ol> <p>I will show the implementation first on Snakemake and latter on Nextflow.</p> <h3 id="snakemake-implementation">Snakemake implementation</h3> <h4 id="setup">Setup</h4> <p>The basic commands for <a href="https://snakemake.readthedocs.io/en/stable/getting_started/installation.html">installing Snakemake</a> are:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda install -n base -c conda-forge mamba
mamba create -c conda-forge -c bioconda -n snakemake snakemake
mamba activate snakemake
snakemake --help
</code></pre></div></div> <p>So, right off the bat, we already need to have Conda installed, and before installing Snakemake, they recommend to install Mamba <strong>using Conda</strong>. If you do this, you might lose a good amount of time. I recommend installing Mamba or Micromamba following their instructions. In my case, I chose to do this with Micromamba, which I started using thanks to a good friend. This is how the commands look:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"${SHELL}" &lt;(curl -L micro.mamba.pm/install.sh)
micromamba create -c conda-forge -c bioconda -n snakemake snakemake
micromamba activate snakemake
</code></pre></div></div> <h4 id="writing-the-workflow">Writing the workflow</h4> <p>First, we will define paths to the FastQC and STAR binaries and other parameters:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>params_fastqc = "Snakemake_Nextflow/FastQC/fastqc"
params_fastqc_memory = "10000"
params_fastqc_threads = 1

params_star_index = "STAR_GenomeIndex/hg38_STAR_2.7.11b"
params_star = "STAR_2.7.11b/Linux_x86_64/STAR"
params_star_threads = 21

params_telescope_gtf = "hg38_all.gtf"
</code></pre></div></div> <p>Next, we will set some variables to guide the different steps of the pipeline. Snakemake allows the use of Python commands, so I’m taking advantage of this. First, the “glob” library is imported, and the “samples” variable is created by listing all files with extension “fq”. Concurrently, through the use of a regular expression, we substitute the trailing _1 or _2 of the paired-end files. In the regular expression we add the quantifier “{0,1}” which makes it also match single-end files that just end on “.fq”, without a trailing _1 or _2.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import glob

samples = list(set([re.sub("(_[12]){0,1}.fq","",x) for x in glob.glob("<span class="err">*</span>.fq")]))
<span class="gh"># ['hg38_tes_random_n1000_simulated_pe_reads_l150_f15_SE', 'hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m1000_s10', 'hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m1100_s10', 'hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m900_s10']</span>
</code></pre></div></div> <p>Similarly, the “fastq_basenames” variable is created:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fastq_basenames = [re.sub(".fq","",x) for x in glob.glob("<span class="err">*</span>.fq")]
<span class="gh"># ['hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m1100_s10_1', 'hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m1000_s10_2', 'hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m1100_s10_2', 'hg38_tes_random_n1000_simulated_pe_reads_l150_f15_SE', 'hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m900_s10_2', 'hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m1000_s10_1', 'hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m900_s10_1']</span>
</code></pre></div></div> <p>The variables “pe_samples” and “se_samples” follow a similar logic, with the exception that the first one will only contain the samples that are paired-end and the second one the ones that are single-end. These variables are used in the function “input_fastqs” which will process a Snakemake input and identify which type is the sample.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pe_samples = list(set([re.sub("(_[12]){0,1}.fq","",x) for x in glob.glob("<span class="err">*</span>_[12].fq")]))
<span class="gh"># ['hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m1000_s10', 'hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m1100_s10', 'hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m900_s10']</span>

se_samples = [re.sub(".fq","",x) for x in glob.glob("<span class="err">*</span>.fq") if re.search("[^_12].fq",x)]
<span class="gh"># ['hg38_tes_random_n1000_simulated_pe_reads_l150_f15_SE']</span>

def input_fastqs(wildcards):
        print(wildcards.sample)
        if wildcards.sample in se_samples:
                return(f'{wildcards.sample}.fq')
        if wildcards.sample in pe_samples:
                return([f'{wildcards.sample}_1.fq',f'{wildcards.sample}_2.fq'])
</code></pre></div></div> <p>Now we can get to the nitty-gritty. The Snakemake logic is that we define rules with inputs and outputs, and it automatically identifies which rule generates the required input of another rule. For example, it is common to create the rule “all” specifying the final outputs of a pipeline. In our case, it looks like this:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rule all:
        input: expand("snakemake_telescope/{sample}_telescope-telescope_report.tsv",sample=samples)
</code></pre></div></div> <p>“samples” is our variable defined above. With “expand” we are telling Snakemake to create a list containing filenames of the form “snakemake_telescope/<strong>{sample}</strong>_telescope-telescope_report.tsv” where sample can be “hg38_tes_random_n1000_simulated_pe_reads_l150_f15_SE” or “hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m1000_s10”. Since those do not exist at the beginning of the pipeline, it will identify the rule that creates those files as outputs. In our case, this is the rule “telescope”:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rule telescope:
        input: "snakemake_star/{sample}Aligned.out.bam"
        output: "snakemake_telescope/{sample}_telescope-telescope_report.tsv"
        conda: "telescope_env"
        shell: "telescope assign --outdir snakemake_telescope --exp_tag {wildcards.sample}_telescope {input} {params_telescope_gtf}"
</code></pre></div></div> <p>As we see, its output are in similar definition to the outputs of our rule “all”. The complete workflow definition is:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rule all:
        input: expand("snakemake_telescope/{sample}_telescope-telescope_report.tsv",sample=samples)

rule readqc:
        input: expand("{fastq_base}.fq",fastq_base=fastq_basenames)
        output: expand("snakemake_fastqc/{fastq_base}_fastqc.html",fastq_base=fastq_basenames)
        shell: "{params_fastqc} --memory {params_fastqc_memory} --threads {params_fastqc_threads} --outdir snakemake_fastqc {input}"

rule map:
        input: input_fastqs
        output: "snakemake_star/{sample}Aligned.out.bam"
        shell: "{params_star} --genomeDir {params_star_index} --runThreadN {params_star_threads} --readFilesIn {input} --outFileNamePrefix snakemake_star/{wildcards.sample} --outFilterMultimapNmax 100 --winAnchorMultimapNmax 100 --outSAMtype BAM Unsorted"

rule telescope:
        input: "snakemake_star/{sample}Aligned.out.bam"
        output: "snakemake_telescope/{sample}_telescope-telescope_report.tsv"
        conda: "telescope_env"
        shell: "telescope assign --outdir snakemake_telescope --exp_tag {wildcards.sample}_telescope {input} {params_telescope_gtf}"
</code></pre></div></div> <p>As discussed above, rule “all” is the main rule defining the final outputs, which are generated by “telescope”, which in turn requires BAM files generated by the “map” rule. Snakemake has the option “–dag”, which allows us to get a quick overview of the rule graph:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/02_nextflow_vs_snakemake/figure02_RNASeq_workflow_dag-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/02_nextflow_vs_snakemake/figure02_RNASeq_workflow_dag-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/02_nextflow_vs_snakemake/figure02_RNASeq_workflow_dag-1400.webp"/> <img src="/assets/img/02_nextflow_vs_snakemake/figure02_RNASeq_workflow_dag.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>It shows us that we have 4 samples processed by the “map” rule, and this goes to “telescope” and finally to “all”. Having this dependency between rules is what makes Snakemake a good option for developing pipelines: if the files are not generated, it will report an error, which we can debug. In contrast, doing this via Bash will entail several conditionals to ensure we are not overwriting successful runs and that we are properly handling interprocess dependencies. When I used Bash, I would end up creating random empty files because one process wouldn’t finish properly, and then the next one would start anyway.</p> <p>We can run the entire workflow like this:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>snakemake --snakefile RNASeq_workflow.smk --cores 1 all readqc --use-conda
</code></pre></div></div> <p>Both the “all” and the “readqc” rule are defined as targets, because we are not actually connecting anything with the FastQC outputs. This is why it appears without connections in the graph. Although we can make it connected to the mapping process, there is not an actual dependency in terms of inputs and outputs.</p> <h3 id="nextflow-implementation">Nextflow implementation</h3> <h4 id="setup-1">Setup</h4> <p>In the <a href="https://www.nextflow.io/">Nextflow homepage</a> it says that it requires “Zero config” and “Just download and play with it. No installation is required”. It requires Java, but other than that it can be set up with:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl -s https://get.nextflow.io | bash
</code></pre></div></div> <p>This doesn’t take more than 2 minutes, and indeed it requires zero installation. It creates a “nextflow” binary in the same directory where the command was run.</p> <h4 id="writing-the-workflow-1">Writing the workflow</h4> <p>Similar to the Snakemake example, I first set up some variables:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>params.fastqc = "Snakemake_Nextflow/FastQC/fastqc"
params.fastqc_memory = "10000"
params.fastqc_threads = 1

params.star_index = "STAR_GenomeIndex/hg38_STAR_2.7.11b"
params.star = "STAR_2.7.11b/Linux_x86_64/STAR"
params.star_threads = 21

params.telescope_gtf = "hg38_all.gtf"
</code></pre></div></div> <p>Instead of rules, we have “processes”. The logic in Nextflow is that a process can take an input and generate an output. The inputs and outputs are called “Channels”, and they can be of any type. For example, the “read_qc” process looks like this:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>process readqc {
        publishDir "nextflow_fastqc"<span class="sb">

        input: path fastq1_files
        output: path "${fastq1_files.baseName}_fastqc.html"

        "$params.fastqc --memory $params.fastqc_memory --threads $params.fastqc_threads $fastq1_files"
</span>}
</code></pre></div></div> <p>whereas the “align” process is:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>process align {
        publishDir "nextflow_star"<span class="sb">

        input: tuple val(basename),file(fastqfiles)
        output: path "${basename}Aligned.out.bam"

        "$params.star --genomeDir $params.star_index --runThreadN $params.star_threads --readFilesIn $fastqfiles --outFileNamePrefix ${basename} --outFilterMultimapNmax 100 --winAnchorMultimapNmax 100 --outSAMtype BAM Unsorted"
</span>}
</code></pre></div></div> <p>and the “telescope” process is:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>process telescope {
        conda '/home/ws2021/miniconda3/envs/telescope_env'
        publishDir "nextflow_telescope"<span class="sb">

        input: path bam_files
        output: val "${bam_files.baseName}_telescope"

        "telescope assign --exp_tag ${bam_files.baseName}_telescope $bam_files $params.telescope_gtf"
</span>}
</code></pre></div></div> <p>So, for “readqc” the inputs are FASTQ files and the outputs are the generated HTML files, which share the basename of the input FASTQ file. Then, in “align” we take as input a tuple, containing the sample identifier, and the FASTQ files (this way we can use the sample identifier for “outFileNamePrefix”), and the output is a BAM file. For “telescope” the input is the BAM file, but here I defined the output as a simple value.</p> <p>The workflow can be defined then as follows:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>workflow {
        PAIRS = Channel.fromFilePairs("<span class="err">*</span>_{1,2}.fq")
        SE = Channel.fromPath("<span class="err">*</span>.fq").filter { it.name =~ /[^12].fq/ }
        SE2 = SE.map { it -&gt; [it.simpleName,it]}<span class="sb">

        readqc(Channel.fromPath("*.fq"))|view
        align(PAIRS.concat(SE2))|telescope
</span>}
</code></pre></div></div> <p>Nextflow seems to have many ways to interact with sequencing data. For example, we can generate two file / path Channels in the first two lines. The first one uses “fromFilePairs” which is perfect for paired-end files. On the second, we use a “fromPath” to consider all files with “.fq” extension, and we filter it to remove those ending with “1” or “2”, effectively keeping single-end files. We process this variable with “map” to tranform it into a tuple:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>PAIRS = Channel.fromFilePairs("<span class="err">*</span>_{1,2}.fq")
<span class="gh">#[hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m1100_s10, [hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m1100_s10_1.fq, Snakemake_Nextflow/hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m1100_s10_2.fq]]</span>
<span class="gh">#[hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m1000_s10, [hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m1000_s10_1.fq, Snakemake_Nextflow/hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m1000_s10_2.fq]]</span>
<span class="gh">#[hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m900_s10, [hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m900_s10_1.fq, Snakemake_Nextflow/hg38_tes_random_n1000_simulated_pe_reads_l150_f20_m900_s10_2.fq]]</span>

SE = Channel.fromPath("<span class="err">*</span>.fq").filter { it.name =~ /[^12].fq/ }
<span class="gh">#hg38_tes_random_n1000_simulated_pe_reads_l150_f15_SE.fq</span>

SE2 = SE.map { it -&gt; [it.simpleName,it]}
<span class="gh">#[hg38_tes_random_n1000_simulated_pe_reads_l150_f15_SE, hg38_tes_random_n1000_simulated_pe_reads_l150_f15_SE.fq]</span>
</code></pre></div></div> <p>This way, we can create a single variable that will allow for the proper processing of reads whether they are single- or paired-end. Altogether, the workflow definition is:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>workflow {
        PAIRS = Channel.fromFilePairs("<span class="err">*</span>_{1,2}.fq")
        SE = Channel.fromPath("<span class="err">*</span>.fq").filter { it.name =~ /[^12].fq/ }
        SE2 = SE.map { it -&gt; [it.simpleName,it]}<span class="sb">

        readqc(Channel.fromPath("*.fq"))|view
        align(PAIRS.concat(SE2))|telescope
</span>}
</code></pre></div></div> <p>Something that I really liked is the ability to pipe processes. Since “align” <em>emits</em> the BAM files required for “telescope”, we can just link them with the pipe.</p> <p>The Nextflow workflow can then be run with:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./nextflow RNASeq_workflow.nf -with-conda
</code></pre></div></div> <h2 id="comparison">Comparison</h2> <p>First things first, I have to say that I enjoyed using both Nextflow and Snakemake, and I wish I would’ve started using them sooner, as they are really convenient. Here, I adapted a really simplified version of what I usually do for RNA-Seq analysis so I could get an understanding on how each framework could be used and implemented. Also, it allowed me to make an unbiased and informed opinion on each one. For example, I didn’t have a very good opinion of Snakemake because according to my previous supervisor, it wasn’t able to use already existing Conda environments. Now I know better because I started to use it properly by carefully reading the documentation, the same way I do with any new thing that I start to learn.</p> <p>If you only plan to analyze sequencing data, I would probably recommend Nextflow because it has built-in features that enhance compatibility and handling this type of data. Setting Nextflow up has to be one of the fastest and hassle-free things that I have done. On the other hand, if you are not familiar with Java, you might face some issues in terms of syntax and how you can manipulate variables. I’m of the idea that once you know how to code in one language, switching to a new one can be fast. Still, I haven’t used Java in years, so I’m a bit rusty. Luckily, <a href="https://www.nextflow.io/docs/latest/script.html">they provide documentation to Groovy</a>, the specific name of the language, which helped me during this comparison. By contrast, in Snakemake you can use Python (which I learned over a year ago), and thus, for the functionalities that were missing I was able to quickly write some code to aid in the pipeline, as shown above.</p> <p>Another misconception that I had was the incompatibility of Nextflow with Conda environments. Nextflow allows for the seamless use and deploying of Conda environments, so if you are familiar with them, you are not really required to use Snakemake.</p> <p>#In terms of lines, the Nextflow script is 39 lines long, while the Snakemake is 43 lines long. I ended up writing helper functions in Snakemake, which take extra space, but in Nextflow I had to add the “publishDir” instruction for it to create and store the resulting files of each process.</p> <p>Probably where Nextflow has the edge is in terms of scalability, outputs and reports. For scalability, its ease of use can be very convenient if you work in a cloud-computing environment or in a computer where you have limited capabilities to install software, such as Conda. I didn’t test it here, but I’m curious of their performance in AWS, for example. In terms of outputs, when you run the pipeline by default, Nextflow only shows the process and the completion percentage. On the other hand, Snakemake prints all the standard output of each rule, so you will see a similar output several times for each file processed in a rule, which will result in a messy terminal. Finally, Nextflow creates a beautiful and comprehensive report <strong>by just adding the -with-report OUTPUT_HTML</strong> option. You can see the report for this workflow <a href="https://bvaldebenitom.github.io/assets/html/">here</a>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/02_nextflow_vs_snakemake/figure03_nextflow_report-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/02_nextflow_vs_snakemake/figure03_nextflow_report-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/02_nextflow_vs_snakemake/figure03_nextflow_report-1400.webp"/> <img src="/assets/img/02_nextflow_vs_snakemake/figure03_nextflow_report.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>With this report you can quickly get an overview of several metrics of importance, such as CPU and memory usage per process and task, and as well as job duration. This can quickly help identify which processes are causing bottlenecks in the pipeline and if a process failed, whether it was caused by a mismatch in allocated memory and peak memory and so on. Overall, this report will go a long way in providing you with the detailed information that will help improve your pipelines.</p> <h2 id="summary">Summary</h2> <p>Here is a short summary of this comparison:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/02_nextflow_vs_snakemake/figure04_summary-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/02_nextflow_vs_snakemake/figure04_summary-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/02_nextflow_vs_snakemake/figure04_summary-1400.webp"/> <img src="/assets/img/02_nextflow_vs_snakemake/figure04_summary.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>I think that both frameworks are excellent and I recommend everyone to give them a try (or at least to one of them). However, given Nextflow’s ease of installation / minimum requirements, the ability to generate amazing reports with just a flag, and more compatibility straight out of the box with sequencing data, it will be my default recommendation in the meantime.</p>]]></content><author><name></name></author><category term="reviews"/><category term="workflows"/><category term="rnaseq"/><summary type="html"><![CDATA[A comparison of Nextflow and Snakemake in the context of RNA-Seq analysis]]></summary></entry><entry><title type="html">Automatization of IGV snapshots</title><link href="https://bvaldebenitom.github.io/blog/2024/igv_coverage/" rel="alternate" type="text/html" title="Automatization of IGV snapshots"/><published>2024-01-06T09:40:00+00:00</published><updated>2024-01-06T09:40:00+00:00</updated><id>https://bvaldebenitom.github.io/blog/2024/igv_coverage</id><content type="html" xml:base="https://bvaldebenitom.github.io/blog/2024/igv_coverage/"><![CDATA[<p>Ever since I started analyzing Transposable Elements (TEs) expression in RNA-Seq data, I have been curious to see the genomic coverage of specific loci. Moreover, when I found out that <a href="https://academic.oup.com/nar/article/47/5/e27/5280934">SQuIRE</a>, one of the best tools at the moment, could generate thousands of false positives, I was more aware of the need to manually verify the expression of TEs. Since there was a large amount of loci to check, I would always postpone this task, as I was never able to find an automated way of generating genomic coverage snapshots. Instead, after several statistical analyses, I would try to shorten the list of TEs to the smallest amount possible, so I could later check them one-by-one on the Integrative Genomics Viewer (IGV) application.</p> <p>It wasn’t until a few weeks ago that I decided to attempt this again. In turn, I was able to find this <a href="https://janbio.home.blog/2020/09/16/igv-batch-snapshot-from-command-line/">blog post</a> with the backbone of the solution. Since then, I modified it to suit my needs for RNA-Seq analyses, and decided to write this post as a more definite tutorial that goes from scratch to the final generation of these long-coveted automated IGV snapshots.</p> <h4 id="1-what-you-will-need">1. What you will need</h4> <ul> <li>Command line IGV 2.16.2 (https://igv.org/)</li> <li>Samtools (https://www.htslib.org/)</li> <li>A genome FASTA file (for this tutorial, I'm using the mm10 FASTA <a href="https://hgdownload.soe.ucsc.edu/goldenPath/mm10/bigZips/">from UCSC</a>)</li> <li>A GTF file (for this tutorial, I'm using the mm10 NCBI RefSeq <a href="https://hgdownload.soe.ucsc.edu/goldenPath/mm10/bigZips/genes/">from UCSC</a>)</li> <li>One or more BAM files (I'm using 2 BAM files here)</li> </ul> <h4 id="2-generating-the-snapshots">2. Generating the snapshots</h4> <p>There are two main steps to actually generating the IGV snapshots in an automated manner. First, we need to index the genome FASTA file, the genome annotation GTF file, and the BAM files.</p> <p>To make things simpler, I recommend creating an environment variable that contains the path to the IGV command line directory:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>export IGV_HOME=/path/IGV_2.16.2/
</code></pre></div></div> <p>Then, to index the genome FASTA file, we can do it like this:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$IGV_HOME/igvtools index mm10.fa
</code></pre></div></div> <p>For the GTF file, we need to sort it first, and then we can run the index command:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$IGV_HOME/igvtools sort mm10.ncbiRefSeq.gtf mm10.ncbiRefSeq_sorted.gtf
$IGV_HOME/igvtools index mm10.ncbiRefSeq_sorted.gtf
</code></pre></div></div> <p><strong>WARNING</strong>: Failure to index the GTF file, will result in long running times. Although the snapshot commands will work, when testing this tutorial on the unindexed file, it went for more than 26 minutes when loading it versus the ~1 minute it will take with the sorted and indexed version.</p> <p>Finally, to index the BAM files, we can use samtools:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>samtools index W8S1PA_Aligned.sortedByCoord.out.bam
samtools index W8S3SM_Aligned.sortedByCoord.out.bam
</code></pre></div></div> <p>The second step is creating the IGV script that will be used to actually generate the snapshots. Here is the template I use now:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gh">#load files</span>
genome mm10.fa 
load mm10.ncbiRefSeq_sorted.gtf 
 
preference SAM.SHOW_ALIGNMENT_TRACK false 

load W8S3SM_Aligned.sortedByCoord.out.bam 
load W8S1PA_Aligned.sortedByCoord.out.bam 
 
<span class="gh">#create snapshots </span>
snapshotDirectory . 

goto chr3:83,764,321-83,776,316 
snapshot igv_demo_sfrp2.png 
 
exit 
</code></pre></div></div> <p>We can save this into the file <code class="language-plaintext highlighter-rouge">igv_snapshot_demo.batch</code>. The script basically consists of loading the genome files (FASTA and sorted GTF), then setting <code class="language-plaintext highlighter-rouge">SAM.SHOW_ALIGNMENT_TRACK</code> to <code class="language-plaintext highlighter-rouge">false</code>, as I don’t usually need to check the reads track, and loading the BAM files. Later, we define the snapshot directory, and through the <code class="language-plaintext highlighter-rouge">goto</code> and <code class="language-plaintext highlighter-rouge">snapshot</code> commands we are actually creating them.</p> <p>Once all the above is done, we can run this final command to efficiently generate all the snapshots in one go:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>xvfb-run --auto-servernum --server-num=1 java -showversion --module-path="${IGV_HOME}/lib" -Xmx8g @"${IGV_HOME}/igv.args" --module=org.igv/org.broad.igv.ui.Main -b igv_snapshot_demo.batch
</code></pre></div></div> <p>In this example, the generated snapshot will be:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/igv_tutorial_sfrp2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/igv_tutorial_sfrp2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/igv_tutorial_sfrp2-1400.webp"/> <img src="/assets/img/igv_tutorial_sfrp2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Snapshot generated for the Sfrp2 gene using the commands in this tutorial. </div> <p>With this, we can now finally generate snapshots for a large number of genomic locations in an automated manner!</p>]]></content><author><name></name></author><category term="tutorials"/><category term="rnaseq"/><summary type="html"><![CDATA[A tutorial on best practices and how to do command line automatization of genomic coverage snapshots using the Integrative Genomics Viewer]]></summary></entry></feed>